# L04 - Cost function

#### Qa Given the following $\mathbf{x}^{(i)}$'s, we have to construct and print the $\mathbf{X}$ matrix in python.

\begin{equation}
\mathbf{x}^{(i)} =
\begin{array}{rl}
\mathbf{x}^{(1)} &= \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} \\
\mathbf{x}^{(2)} &= \begin{bmatrix} 4 \\ 2 \\ 1 \end{bmatrix} \\
\mathbf{x}^{(3)} &= \begin{bmatrix} 3 \\ 8 \\ 5 \end{bmatrix} \\
\mathbf{x}^{(4)} &= \begin{bmatrix} -9 \\ -1 \\ 0 \end{bmatrix}
\end{array}
\end{equation}
Using np.array and without using built-in python lists or the numpy matrix subclass

Since we have to use np.array the given column vectors are simply defined using np.array as shown in the code and np.vstack is then used to construct the full matrix since it is given that we should not construct using python lists.

Implement the $\mathcal{L}_1$ and $\mathcal{L}_2$ norms for vectors in python.
We first have to make a low-level implementation of the $\mathcal{L}_1$ and $\mathcal{L}_2$ norms using only ```+```, ```*``` and power ```**```. Without using any libraries or build in python functions. We then have to test the implementations against build in functions like ```numpy.linalg.norm```. Finally we have to optimize $\mathcal{L}_2$ such that it uses the matrix multiplication operator instead of an explicit sum and call this function ```L2MatrixMult```. Python functions are here allowed but it must be pythonic.

We start by creating the low level implementation using while loops. In the L1 implementation we note that we have to make sure we get the absolute value which is done using an if statement. This is not necessary in the L2 implementation since the product is summed but we take the square root of the sum at the end. These implementation are then tested against ```numpy.linalg.norm```, producing no difference as can be seen in the print.

The L2 function is then optimized using the matrix multiplication operator @, which means we can simply use the dot product implementation, which can be directly calculated as q @ q and then find the square root.

As can be seen below, the implementation of both the low level function and the optimized version passes the tests.

### Qc Construct the Root Mean Square Error (RMSE) function (Equation 2-1 [HOML]).
We have to construct an RMSE function and evaluate it based on the $\mathbf{X}$ matrix and $\mathbf{y}$ from Qa.

The RMSE function can be constructed based on the L2 function from Qb, since RMSE is defined as:
\begin{equation*}
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (\hat{Y}_i - Y_i)^2 \\
\end{equation*}

\begin{equation*}
\text{RMSE} = \sqrt{\text{MSE}}
\end{equation*}

Where
\begin{equation*}
\text{L2} = \sqrt{(\hat{Y}_i - Y_i)^2}
\end{equation*}

Which gives
\begin{equation*}
\text{RMSE} = \frac{1}{\sqrt{n}} \,\text{L2}
\end{equation*}

This can then be implemented as shown in the code below which produces a result that is the same as the expected to the given decimals.

#### Qd Similar construct the Mean Absolute Error (MAE) function (Equation 2-2 [HOML]) and evaluate it.
We similarly have to construct the MAE using $\mathcal{L}_1$ from Qb.

MAE is the mean absolute error defined as:
\begin{equation*}
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} \lvert \hat{Y}_i - Y_i \rvert
\end{equation*}
Where
\begin{equation*}
\text{L1} = \sum_{i=1}^{n} \lvert \hat{Y}_i - Y_i \rvert
\end{equation*}
Thus
\begin{equation*}
\text{MAE} = \frac{1}{n} \,\text{L1}
\end{equation*}

Which can then be implemented and thereby, as expected, produces a difference of zero

#### Qe Robust Code 
We have to add asserts or exceptions that checks the $\hat{ \mathbf{y}}$-$ \mathbf{y} $ sizes of the MSE and MAE function. We also have to add error checking to all the previously tested L2() and L1() functions.

To checks that the sizes of the input vectors in the MSE and MAE function are correct, an assert is added at the beginning which checks that the vectors are one dimensional and of the same size as:
```
assert x.shape[0]>0 and x.ndim==1 and x.shape[0]==y.shape[0], "Error in vectors"
```
This is added to the previous code as instructed and the tests are re-run.

For the previously tested L2() and L1() functions and assert is similarly added to ensure the input is a vector, as is expected.
```
assert x.shape[0]>0 and x.ndim==1, "Not a 1-D vector"
```

It should be noted that a check for column and row vectors is also often used but because the definitions of which are used in this context are a bit fuzzy, for example is the y_true vector described as column, but given as a row, this check is excluded here.
